{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab4c9ee4-80b8-4bb1-acf9-ad6d9ad78ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, hash, count, lit\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31f6d1ff-3eaf-4a9f-aa82-a832114377da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the connection properties to SQL Server\n",
    "jdbcHostname = \"COGNINE-L143\"\n",
    "jdbcPort = 1433\n",
    "jdbcDatabase = \"Data\"  # Replace with your database name\n",
    "jdbcUrl = f\"jdbc:sqlserver://{jdbcHostname}:{jdbcPort};databaseName={jdbcDatabase};trustServerCertificate=true\"\n",
    "connectionProperties = {\n",
    "    \"user\": \"Read\",  # Replace with your username\n",
    "    \"password\": \"Welcome2cognine\",  # Replace with your password\n",
    "    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14490c85-38df-43e6-9c06-1730f37fc9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the JDBC driver JAR file\n",
    "jdbc_driver_path = \"C:\\\\sqljdbc_12.8\\\\enu\\\\jars\\\\mssql-jdbc-12.8.0.jre11.jar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5027e2c-c134-43f2-a266-e46258dcb596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Spark session with the JDBC driver\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"StructuredStreamingWithSalting\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", jdbc_driver_path) \\\n",
    "    .config(\"spark.executor.extraClassPath\", jdbc_driver_path) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54044ac8-1a7a-4509-9bce-6d3ee5e6c8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_sql(max_id):\n",
    "    query = f\"(SELECT * FROM [220k_awards_by_directors] WHERE ID > {max_id}) AS data\"  # Replace with your table and query\n",
    "    df = spark.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", jdbcUrl) \\\n",
    "        .option(\"dbtable\", query) \\\n",
    "        .option(\"user\", connectionProperties[\"user\"]) \\\n",
    "        .option(\"password\", connectionProperties[\"password\"]) \\\n",
    "        .option(\"driver\", connectionProperties[\"driver\"]) \\\n",
    "        .load()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93f5e954-f2e4-4556-8bb6-9dacf180c1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize max_id\n",
    "max_id = 0\n",
    "save_file_number = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f29f153-65d3-4b21-852e-6982d26cf209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base output path for the processed data\n",
    "base_output_path = r\"C:\\Users\\Manideep S\\OneDrive - COGNINE\\ML\\Assessments\\PySpark\\DataSkew-Salting\"  # Replace with the desired base output path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac1f51aa-a8c6-46a5-a9ae-8e725d546b1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m     save_file_number \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Sleep for a specific interval before the next read\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Start streaming loop\n",
    "while True:\n",
    "    # Read the latest data from SQL Server\n",
    "    new_data = read_from_sql(max_id)\n",
    "    \n",
    "    if new_data.count() > 0:\n",
    "        # Add a salt column to the DataFrame using a hash of the director_name column\n",
    "        salted_df = new_data.withColumn(\"salt\", (hash(col(\"director_name\")) % 10).cast(\"string\"))\n",
    "        \n",
    "        # Perform processing tasks on salted_df\n",
    "        # Example processing task 1: Filter rows where outcome is 'Won'\n",
    "        filtered_df = salted_df.filter(col(\"outcome\") == \"Won\")\n",
    "        \n",
    "        # Example processing task 2: Group by director_name and count the number of awards\n",
    "        aggregated_df = filtered_df.groupBy(\"director_name\").agg(count(\"*\").alias(\"award_count\"))\n",
    "        \n",
    "        # Example task: Add a new column that converts 'award_count' to string\n",
    "        processed_df = aggregated_df.withColumn(\"award_count_str\", col(\"award_count\").cast(\"string\"))\n",
    "        \n",
    "        # Define the output path for the current file\n",
    "        output_path = f\"{base_output_path}_{save_file_number}.parquet\"\n",
    "        \n",
    "        # Write the processed DataFrame to a Parquet file\n",
    "        processed_df.write.mode(\"overwrite\").parquet(output_path)\n",
    "        \n",
    "        # Update max_id to the maximum ID from the newly read data\n",
    "        max_id = new_data.agg({\"ID\": \"max\"}).collect()[0][0]\n",
    "        \n",
    "        # Increment the file number for the next save\n",
    "        save_file_number += 1\n",
    "    \n",
    "    # Sleep for a specific interval before the next read\n",
    "    time.sleep(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ecc1e14b-55fd-4625-b136-66059d8ccb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "parquet_dir = r'C:\\Users\\Manideep S\\OneDrive - COGNINE\\ML\\Assessments\\PySpark'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68b62efc-36a7-4ac1-8844-5b0563cc8495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all Parquet files into a list of DataFrames\n",
    "parquet_files = [f\"{parquet_dir}/{file}\" for file in os.listdir(parquet_dir) if file.endswith(\".parquet\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "073a3578-0bcb-41e6-85eb-7230bcc2da46",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = [spark.read.parquet(file) for file in parquet_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7a8d202-5de6-49a1-bc40-4d6ba1a6fc54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "root\n",
      " |-- director_name: string (nullable = true)\n",
      " |-- award_count: long (nullable = true)\n",
      " |-- award_count_str: string (nullable = true)\n",
      "\n",
      "number of partitions:  1\n",
      "___________________________\n",
      "1\n",
      "root\n",
      " |-- director_name: string (nullable = true)\n",
      " |-- award_count: long (nullable = true)\n",
      " |-- award_count_str: string (nullable = true)\n",
      "\n",
      "number of partitions:  1\n",
      "___________________________\n",
      "2\n",
      "root\n",
      " |-- director_name: string (nullable = true)\n",
      " |-- award_count: long (nullable = true)\n",
      " |-- award_count_str: string (nullable = true)\n",
      "\n",
      "number of partitions:  1\n",
      "___________________________\n",
      "3\n",
      "root\n",
      " |-- director_name: string (nullable = true)\n",
      " |-- award_count: long (nullable = true)\n",
      " |-- award_count_str: string (nullable = true)\n",
      "\n",
      "number of partitions:  1\n",
      "___________________________\n",
      "4\n",
      "root\n",
      " |-- director_name: string (nullable = true)\n",
      " |-- award_count: long (nullable = true)\n",
      " |-- award_count_str: string (nullable = true)\n",
      "\n",
      "number of partitions:  1\n",
      "___________________________\n",
      "5\n",
      "root\n",
      " |-- director_name: string (nullable = true)\n",
      " |-- award_count: long (nullable = true)\n",
      " |-- award_count_str: string (nullable = true)\n",
      "\n",
      "number of partitions:  1\n",
      "___________________________\n",
      "6\n",
      "root\n",
      " |-- director_name: string (nullable = true)\n",
      " |-- award_count: long (nullable = true)\n",
      " |-- award_count_str: string (nullable = true)\n",
      "\n",
      "number of partitions:  1\n",
      "___________________________\n"
     ]
    }
   ],
   "source": [
    "c=0\n",
    "for df in dataframes:\n",
    "    print(c)\n",
    "    df.printSchema()\n",
    "    print(\"number of partitions: \",df.rdd.getNumPartitions())\n",
    "    print(\"___________________________\")\n",
    "    c=c+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b967099f-1d89-4e0a-a077-41c6102d6836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all DataFrames into a single DataFrame\n",
    "combined_df = dataframes[0]\n",
    "for df in dataframes[1:]:\n",
    "    combined_df = combined_df.union(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ecf2654d-b50e-4934-95e4-6d30e5ed6580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- director_name: string (nullable = true)\n",
      " |-- award_count: long (nullable = true)\n",
      " |-- award_count_str: string (nullable = true)\n",
      "\n",
      "+------------------+-----------+---------------+\n",
      "|     director_name|award_count|award_count_str|\n",
      "+------------------+-----------+---------------+\n",
      "|    Allison Anders|          7|              7|\n",
      "|      Jim Jarmusch|         30|             30|\n",
      "|        Rob Bowman|          1|              1|\n",
      "|       John Milius|          3|              3|\n",
      "|     Marcel Ophüls|         11|             11|\n",
      "|     Paul Borghese|          5|              5|\n",
      "|      Wayne Kramer|          1|              1|\n",
      "|Pierre-Alain Meier|          1|              1|\n",
      "|Christopher Monger|          7|              7|\n",
      "|    Josef von Báky|          1|              1|\n",
      "|       Tommy Chong|          1|              1|\n",
      "|        Bobby Roth|          3|              3|\n",
      "|  Terry Cunningham|          1|              1|\n",
      "|     Robert Parigi|          3|              3|\n",
      "|     Anne Goursaud|          2|              2|\n",
      "|   Kwak Kyung-taek|          6|              6|\n",
      "|     Neeraj Pandey|          3|              3|\n",
      "|  Laurence Olivier|         38|             38|\n",
      "|Heidi Maria Faisst|          3|              3|\n",
      "|   Michel Boujenah|          3|              3|\n",
      "+------------------+-----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the schema and some rows to check the data\n",
    "combined_df.printSchema()\n",
    "combined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7bee18bd-e838-4afe-9ff5-330afbbc0d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DataFrame has 7 partitions.\n"
     ]
    }
   ],
   "source": [
    "# Check for partitions\n",
    "if combined_df.rdd.getNumPartitions() > 1:\n",
    "    print(f\"The DataFrame has {combined_df.rdd.getNumPartitions()} partitions.\")\n",
    "else:\n",
    "    print(\"The DataFrame has only one partition.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
